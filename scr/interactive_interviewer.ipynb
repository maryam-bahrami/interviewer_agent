{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5b6e74-f49e-48e8-9886-444620086713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/site-packages (0.9.9)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/site-packages (1.0.8)\n",
      "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/site-packages (from dotenv) (1.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /usr/local/lib/python3.11/site-packages (from langchain) (1.0.6)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.11/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.11/site-packages (from langchain-openai) (2.8.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.43)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8b981e-720b-46df-b0a9-c7d6af53b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Dict, Optional, TypedDict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.chat_models import init_chat_model\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import pathlib\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "load_dotenv(os.path.join(parent_dir, \".env\"))\n",
    "\n",
    "def load_job_config(cfg_path):\n",
    "  with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "@dataclass\n",
    "class JobConfig:\n",
    "    jd: str\n",
    "    questions: list\n",
    "    q_idx: int\n",
    "    latest_answer: list\n",
    "    pending_followups: list\n",
    "    last_prompt: str\n",
    "    answers: list\n",
    "    no_followup_chances: int\n",
    "    done: bool\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    jd: str\n",
    "    questions: List[Dict]\n",
    "    q_idx: int\n",
    "    latest_answer: Optional[str]\n",
    "    pending_followups: List[str]\n",
    "    last_prompt: Optional[str]\n",
    "    answers: List[Dict]\n",
    "    llm_responses: List[Dict]\n",
    "    done: bool\n",
    "    report: str\n",
    "    review: str\n",
    "\n",
    "\n",
    "\n",
    "# ---- Helpers to seed initial state -----------------------------------------\n",
    "\n",
    "def initial_state_from_config(config: JobConfig) -> AgentState:\n",
    "    \"\"\"Create the initial LangGraph state from the loaded job configuration.\"\"\"\n",
    "    return AgentState(\n",
    "        jd=config.jd,\n",
    "        questions=config.questions,\n",
    "        q_idx=0,\n",
    "        latest_answer=None,\n",
    "        pending_followups=[],\n",
    "        last_prompt=None,\n",
    "        answers=[],\n",
    "        llm_responses=[],\n",
    "        review = \"\",\n",
    "        done=False,\n",
    "    )\n",
    "    \n",
    "# Path to the job configuration JSON file (relative to this script)\n",
    "cfg_path = \"data/job_config.json\"\n",
    "\n",
    "# Load job description and interview questions\n",
    "config = load_job_config(cfg_path)\n",
    "\n",
    "config = JobConfig(\n",
    "    jd=config[\"job_description\"],\n",
    "    questions=config[\"questions\"],\n",
    "    q_idx=0,  # Start state\n",
    "    latest_answer=None,\n",
    "    pending_followups=[],\n",
    "    last_prompt=None,\n",
    "    answers=[],\n",
    "    no_followup_chances=int(config[\"number_of_followup_chances\"]),\n",
    "    done=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9cdbda-98cf-40f3-8111-e70a22de36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "class Interviewer:\n",
    "    \"\"\"Encapsulates the ask/evaluate logic as class methods.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chat_model = init_chat_model(\n",
    "                                        os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\"),\n",
    "                                        base_url=os.getenv(\"BASE_URL\"),\n",
    "                                        api_key=os.getenv(\"API_KEY\", \"not-needed\"),\n",
    "                                           )\n",
    "\n",
    "    async def node_ask_question(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Async ask node that prompts the user and waits for their answer.\"\"\"\n",
    "        import asyncio\n",
    "\n",
    "        if state.get(\"done\"):\n",
    "            return state\n",
    "\n",
    "        # Prioritize follow‑ups\n",
    "        if state.get(\"pending_followups\"):\n",
    "            prompt = state[\"pending_followups\"].pop(0)\n",
    "        else:\n",
    "            q_idx = state.get(\"q_idx\", 0)\n",
    "            questions = state[\"questions\"]\n",
    "            if q_idx >= len(questions):\n",
    "                state[\"done\"] = True\n",
    "                state[\"last_prompt\"] = None\n",
    "                return state\n",
    "            prompt = questions[q_idx][\"text\"]\n",
    "\n",
    "        # Show the prompt and wait for user input asynchronously\n",
    "        print(prompt)\n",
    "        answer = await asyncio.to_thread(lambda: input(\"> \"))\n",
    "        # Store the answer so the evaluation node can process it\n",
    "        state[\"latest_answer\"] = answer\n",
    "        state[\"last_prompt\"] = prompt\n",
    "\n",
    "        return state\n",
    "\n",
    "    def node_evaluate_answer(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Evaluate the latest answer, generate follow-ups, and advance the state.\"\"\"\n",
    "        if state.get(\"done\"):\n",
    "            return state\n",
    "    \n",
    "        if state.get(\"latest_answer\") is None:\n",
    "            return state\n",
    "    \n",
    "        q_idx = state.get(\"q_idx\", 0)\n",
    "        questions = state[\"questions\"]\n",
    "        if q_idx >= len(questions):\n",
    "            state[\"done\"] = True\n",
    "            return state\n",
    "    \n",
    "        latest = (state.get(\"latest_answer\") or \"\").strip()\n",
    "        q = questions[q_idx]\n",
    "        question = q[\"text\"]\n",
    "        required_keywords = q.get(\"required_keywords\", [])\n",
    "    \n",
    "        # ----- LLM CALL -----------------------------------------------------------\n",
    "        system_prompt = \"\"\"\n",
    "        You are an expert hiring assistant evaluating candidate answers during an interview. Your task is to check whether the candidate’s answer demonstrates understanding of the topic. Understanding may be shown in two ways: 1. The answer explicitly contains the expected keywords. 2. The answer does not use the exact keywords but explains the concepts correctly and completely. Always evaluate based on meaning, not just exact wording. Return your result in the specified JSON structure and be kind with the user.\n",
    "        Return ONLY valid JSON.\n",
    "        \"\"\"\n",
    "    \n",
    "        user_prompt = \"\"\"\n",
    "        [Interview Question]\n",
    "        {question}\n",
    "    \n",
    "        [Expected Keywords or Concepts]\n",
    "        {required_keywords}\n",
    "    \n",
    "        [Candidate Answer]\n",
    "        {latest}\n",
    "    \n",
    "        Your task: \n",
    "        1. Identify whether the candidate’s answer contains each expected keyword. \n",
    "        2. If a keyword is missing but the candidate clearly explains the idea, mark it as \"explained\". \n",
    "        3. If neither the keyword nor the concept is present, mark it as \"missing\". \n",
    "        4. Give a short explanation for each classification. \n",
    "        5. If a keyword is missing make one follow-up question to clarify the missing keywords in \"follow_up\" \n",
    "        6. If the user does not understand the question, try to clarify and reformulate the question.\n",
    "        7. If the user asks question about company data, do not provide the the data and reject the user's request respectfully.\n",
    "        8. If the user provides any personal data (e.g age, gender, marital status, address etc.), do not store this data in the memory.\n",
    "        9. Do not ask user any discriminative questions (e.g gender, nationality, color of skin etc.)\n",
    "        10. If a question was answered previously, do not ask it again. \n",
    "        \n",
    "        JSON format:\n",
    "        {{\n",
    "          \"keywords\": [\n",
    "            {{\n",
    "              \"keyword\": \"\",\n",
    "              \"status\": \"present | explained | missing\",\n",
    "              \"explanation\": \"\"\n",
    "            }}\n",
    "          ],\n",
    "          \"overall_assessment\": \"\",\n",
    "          \"score\": \"\",\n",
    "          \"follow_up\": \"\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = user_prompt.format(question = q[\"text\"],\n",
    "                                           required_keywords=required_keywords,\n",
    "                                           latest=latest)\n",
    "        \n",
    "        llm_response = self.chat_model.invoke([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_prompt),\n",
    "        ])\n",
    "        parsed = json.loads(llm_response.content)\n",
    "    \n",
    "        # ---- TRACK LLM RESPONSES -------------------------------------------------\n",
    "        if \"llm_responses\" not in state:\n",
    "            state[\"llm_responses\"] = []\n",
    "    \n",
    "        # initialize response tracker for this question if needed\n",
    "        if len(state[\"llm_responses\"]) <= q_idx:\n",
    "            state[\"llm_responses\"].append({\n",
    "                \"q_idx\": q_idx,\n",
    "                \"follow_up_count\": 0,\n",
    "                \"history\": []\n",
    "            })\n",
    "    \n",
    "        q_track = state[\"llm_responses\"][q_idx]\n",
    "        q_track[\"history\"].append(parsed)\n",
    "    \n",
    "        follow_up = parsed.get(\"follow_up\", \"\").strip()\n",
    "    \n",
    "        # ---- FOLLOW-UP LOGIC WITH LIMIT -------------------------------------\n",
    "        if follow_up:  \n",
    "            if q_track[\"follow_up_count\"] < config.no_followup_chances :\n",
    "                q_track[\"follow_up_count\"] += 1\n",
    "                state[\"pending_followups\"].append(follow_up)\n",
    "            else:\n",
    "                # exceeded 3 follow-ups → move to next question\n",
    "                #print(\"⚠️ Maximum follow-ups reached. Moving to next question.\")\n",
    "                state[\"pending_followups\"] = []\n",
    "                state[\"q_idx\"] = q_idx + 1\n",
    "        else:\n",
    "            # answer was complete → move to next question\n",
    "            state[\"q_idx\"] = q_idx + 1\n",
    "    \n",
    "        # ---- SAVE USER ANSWER ----------------------------------------------------\n",
    "        answers = state.get(\"answers\", [])\n",
    "        answers.append({\n",
    "            \"question_id\": q[\"id\"],\n",
    "            \"question\": q[\"text\"],\n",
    "            \"answer\": latest,\n",
    "            \"notes\": q.get(\"guidance\", \"\")\n",
    "        })\n",
    "        state[\"answers\"] = answers\n",
    "    \n",
    "        #state[\"latest_answer\"] = None\n",
    "\n",
    "        return state\n",
    "            \n",
    "    def node_reviewer(self, state: AgentState) -> AgentState:\n",
    "        reviewer_prompt = \"\"\"\n",
    "                You are an expert AI-engineering interviewer reviewing candidate answers.\n",
    "                Evaluate all answers based on:\n",
    "                \n",
    "                **1. Relevance to the question**\n",
    "                **2. Technical correctness**\n",
    "                **3. Alignment with the job description (JD)**\n",
    "                **4. Presence of required keywords (from question metadata)**\n",
    "                **5. Professionalism and clarity**\n",
    "                **6. Depth of experience**\n",
    "                **7. Signal vs noise (usefulness)**\n",
    "                \n",
    "                ---------------------------\n",
    "                ### JOB DESCRIPTION:\n",
    "                {jd}\n",
    "                \n",
    "                ---------------------------\n",
    "                ### CANDIDATE ANSWERS:\n",
    "                Provide a structured review for **each answer** below.\n",
    "                \n",
    "                Format per answer:\n",
    "                - **Question ID**\n",
    "                - **Question Text**\n",
    "                - **Candidate Answer**\n",
    "                - **Evaluation** (2–5 sentences)\n",
    "                - **Score (0–10)** based on relevance + technical depth + JD alignment\n",
    "                - **Keyword Coverage**: list which required keywords (from question.required_keywords) are present or missing\n",
    "                \n",
    "                Finally produce:\n",
    "                \n",
    "                ### OVERALL SUMMARY\n",
    "                - Strengths\n",
    "                - Weaknesses\n",
    "                - Hiring Risk Level (Low/Medium/High)\n",
    "                - Final Overall Score (0–10)\n",
    "                \n",
    "                ---------------------------\n",
    "                ### DATA:\n",
    "                {answers}\n",
    "                ---------------------------\n",
    "                \n",
    "                Return JSON with this structure:\n",
    "                \n",
    "                {{\n",
    "                  \"per_question_reviews\": [...],\n",
    "                  \"overall_summary\": \"...\",\n",
    "                  \"final_score\": <number 0–10>\n",
    "                }}\n",
    "                \"\"\"\n",
    "        reviewer_prompt = reviewer_prompt.format(jd=state[\"jd\"], answers=state[\"answers\"])\n",
    "        response = self.chat_model.invoke([HumanMessage(content=reviewer_prompt)])\n",
    "        state[\"review\"] = response.content  # attach review to state\n",
    "\n",
    "        return state\n",
    "\n",
    "    def node_reporter(self, state: AgentState) -> AgentState:\n",
    "        reporter_prompt = \"\"\"\n",
    "                    You are an expert technical writer.  \n",
    "                    Your task is to convert the review data into a **clean, polished, executive-quality Markdown report**.\n",
    "                    \n",
    "                    The audience is:\n",
    "                    - Hiring managers\n",
    "                    - Senior AI/ML engineers\n",
    "                    - Talent acquisition specialists\n",
    "                    \n",
    "                    The tone should be:\n",
    "                    - Professional\n",
    "                    - Clear\n",
    "                    - Concise\n",
    "                    - Evidence-based\n",
    "                    \n",
    "                    ---------------------------\n",
    "                    ### REVIEW DATA TO SUMMARIZE:\n",
    "                    \n",
    "                    {review}\n",
    "                    \n",
    "                    ---------------------------\n",
    "                    \n",
    "                    ### MARKDOWN REPORT REQUIREMENTS\n",
    "                    \n",
    "                    Produce a Markdown document with the following structure:\n",
    "                    \n",
    "                    # Candidate Evaluation Report\n",
    "                    \n",
    "                    ## 1. Executive Summary\n",
    "                    - One concise paragraph summarizing overall performance, strengths, and concerns.\n",
    "                    \n",
    "                    ## 2. Job Description Alignment\n",
    "                    Summarize how well the candidate matches the JD requirements.\n",
    "                    \n",
    "                    ## 3. Detailed Review by Question\n",
    "                    For each question:\n",
    "                    - **Question ID**\n",
    "                    - **Question Text**\n",
    "                    - **Score**\n",
    "                    - **Summary of evaluation**\n",
    "                    - **Keyword Coverage**\n",
    "                    - Bullet points highlighting strengths and weaknesses.\n",
    "                    \n",
    "                    ## 4. Overall Assessment\n",
    "                    - Final score (0–10)\n",
    "                    - Hiring recommendation: **Strong Hire / Hire / Weak Hire / No Hire**\n",
    "                    \n",
    "                    ## 5. Risks & Flags\n",
    "                    Bullet list of any major issues or concerns.\n",
    "                    \n",
    "                    Ensure the Markdown is clean and does not include unnecessary JSON dumps.\n",
    "                    \"\"\"\n",
    "                            \n",
    "        reporter_prompt = reporter_prompt.format(review=state[\"review\"])\n",
    "        response = self.chat_model.invoke([HumanMessage(content=reporter_prompt)])\n",
    "        state[\"report\"] = response.content  # attach report to state\n",
    "        output_path = pathlib.Path(\"report.md\")\n",
    "        output_path.write_text(state[\"report\"], encoding=\"utf-8\")\n",
    "        \n",
    "        print(\"******\")\n",
    "        pprint(state[\"report\"])\n",
    "        print(\"******\")\n",
    "        return state\n",
    "        \n",
    "    def router(self, state: AgentState) -> str:\n",
    "        \"\"\"Decide the next node based on the current state.\"\"\"\n",
    "        if state.get(\"done\"):\n",
    "            #return \"end\"\n",
    "            return \"review\"\n",
    "        if state.get(\"pending_followups\"):\n",
    "            return \"ask\"\n",
    "        # If we have just recorded an answer (latest_answer cleared) we need to ask next\n",
    "        return \"ask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87ce856-bcc5-4f23-86ae-e9bef763afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Graph builder -------------------------------------------------------\n",
    "\n",
    "def build_graph(config: JobConfig) -> StateGraph:\n",
    "    \"\"\"Create a LangGraph where the interview flow is driven entirely by the graph.\"\"\"\n",
    "    interviewer = Interviewer()\n",
    "\n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # Register the class methods as graph nodes\n",
    "    builder.add_node(\"ask\", interviewer.node_ask_question)\n",
    "    builder.add_node(\"evaluate\", interviewer.node_evaluate_answer)\n",
    "    builder.add_node(\"review\", interviewer.node_reviewer)\n",
    "    builder.add_node(\"report\", interviewer.node_reporter)\n",
    "\n",
    "    # Entry point is the evaluation node – it will immediately route to “ask”\n",
    "    builder.set_entry_point(\"evaluate\")\n",
    "\n",
    "    # After evaluation decide where to go next\n",
    "    builder.add_conditional_edges(\n",
    "        \"evaluate\",\n",
    "        interviewer.router,\n",
    "        {\n",
    "            \"ask\": \"ask\",\n",
    "            #\"end\": END\n",
    "            \"review\": \"review\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # After asking a question we always go back to evaluation (once the UI supplies an answer)\n",
    "    builder.add_edge(\"ask\", \"evaluate\")\n",
    "    builder.add_edge(\"review\", \"report\")\n",
    "    builder.add_edge(\"report\", END)\n",
    "    \n",
    "    memory = MemorySaver()\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    return graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cf7c4-3783-4786-90c7-9aa6a75d6b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you describe your experience building agentic workflows? What tools and patterns did you use?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  HOw many peOple are wOrking in the COmpany.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you describe your experience building agentic workflows, including the tools you used (e.g., LangGraph) and the patterns such as planning, reflection, and multi‑agent coordination?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  dfvbdfbdafbs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about your experience deploying on Azure for GenAI apps.\n"
     ]
    }
   ],
   "source": [
    "state = initial_state_from_config(config)\n",
    "graph = build_graph(config)\n",
    "result_state = await graph.ainvoke(state, {\"configurable\": {\"thread_id\": \"ui\", \"recursion_limit\": 100}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aebe1d-a195-4a76-83fd-b0484b29d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfadd18-eabb-4fa8-9ea5-cc2258527d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86514b0-a8ea-4e0b-be4c-2f97f49f91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_reviewer(self, state: AgentState) -> AgentState:\n",
    "\n",
    "        \n",
    "        \n",
    "        system_prompt = \"\"\"\n",
    "        You are an expert answer reviewer.\n",
    "        Your job:\n",
    "        - Evaluate how well a candidate's answer responds to a given question.\n",
    "        - Compare the answer against a list of expected points or keywords.\n",
    "        - Be strict but fair, and explain your reasoning briefly.\n",
    "        - Never invent facts that are not in the answer.\n",
    "        \n",
    "        Evaluation criteria:\n",
    "        1. Relevance – Does the answer actually address the question?\n",
    "        2. Completeness – How many of the expected points are covered?\n",
    "        3. Depth – Does the answer show understanding, not just buzzwords?\n",
    "        4. Clarity – Is the answer clear and coherent?\n",
    "        \n",
    "        Return your evaluation **only** in this JSON format:\n",
    "        \n",
    "        {\n",
    "          \"score\": 0–100,\n",
    "          \"verdict\": \"excellent\" | \"good\" | \"average\" | \"poor\",\n",
    "          \"covered_points\": [ \"point1\", \"point2\", ... ],\n",
    "          \"missing_points\": [ \"pointX\", \"pointY\", ... ],\n",
    "          \"strengths\": \"short paragraph\",\n",
    "          \"weaknesses\": \"short paragraph\",\n",
    "          \"follow_up_question\": \"one concise follow-up question focusing on gaps\"\n",
    "        }\n",
    "        \n",
    "        If something is not applicable, use an empty list or an empty string.\n",
    "        Do not include any other text outside the JSON.\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        Question:\n",
    "        {question}\n",
    "        \n",
    "        Candidate answer:\n",
    "        {answer}\n",
    "        \n",
    "        Expected points to look for (can be keywords, concepts, or examples):\n",
    "        {expected_points}\n",
    "        \n",
    "        Please review the answer based on the system instructions and return the JSON evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "        llm_response = call_llm(system_prompt, user_prompt)\n",
    "        state[\"llm_response\"] = llm_response.content\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807bd79e-4c8d-4ec7-939f-ecc41922cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdf669-190a-4832-a0c9-20607e4e39bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "dadc6b2a-c581-41b9-ab19-939b5a8c6ca1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
